[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. student in the Computer Science department at the University of California, San Diego working with Prof. Jingbo Shang. I am broadly interested in Machine Learning and Natural Language Processing.\nFor summer 2021, I am interning at Amazon Science with Dr. Xin Luna Dong. I completed my Bachelor Of Technology in Computer Science And Engineering from the Indian Institute of Technology, Kanpur in 2017, where I worked with Prof. Harish Karnick and Prof. Purushottam Kar. I worked as a Data Scientist and Product Engineer at Sprinklr for 2 years and I interned at Microsoft India in the summer of 2016.\nCurrent Research Few Shot \u0026amp; Weakly Supervised Learning I develop high performing deep neural frameworks with minimal human supervision such as just class labels or a few label-indicative seed words. I am interested in leveraging massive amounts of unstructured and unlabeled data available on the internet for supervision and additional contextual information. Further, I am also keen on beneficially leveraging pre-trained language models to reduce the need for annotated data.\nSecurity I study vulnerabilities of current NLP systems such as data poisoning and trigger-based backdoor attacks and work towards developing strong defense methods against such attacks.\nDeep Learning A deep neural network is known to learn/overfit any randomly labeled data. I am interested in understanding and unveiling the learning process of deep neural architectures and further use it to analyze the quality of data.\nApart from Academics, I enjoy spending time playing Ukulele, playing Football(soccer) and I rarely write too. Checkout my blog!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://schopra6.github.io/author/dheeraj-mekala/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dheeraj-mekala/","section":"authors","summary":"I am a Ph.D. student in the Computer Science department at the University of California, San Diego working with Prof. Jingbo Shang. I am broadly interested in Machine Learning and Natural Language Processing.","tags":null,"title":"Sahil Chopra","type":"authors"},{"authors":[],"categories":[],"content":"Ad-hoc Document Retrieval using WeakSupervision with BERT and GPT2 ","date":1619548705,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619548705,"objectID":"f415122149049d1b10d593b0dcbfb327","permalink":"https://schopra6.github.io/post/mar_2021_nlp/","publishdate":"2021-04-27T11:38:25-07:00","relpermalink":"/post/mar_2021_nlp/","section":"post","summary":"The blog post summarizing a few papers from EMNLP 2020 and some recent papers that I have enjoyed reading in March 2021.","tags":[],"title":"March 2021 NLP Reading: ","type":"post"},{"authors":["Zihan Wang","Sahil Chopra","Jingbo Shang"],"categories":null,"content":"","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615420800,"objectID":"daf1a7b57635ad7cda1447c3592f0583","permalink":"https://schopra6.github.io/publication/xclass/","publishdate":"2021-03-11T00:00:00Z","relpermalink":"/publication/xclass/","section":"publication","summary":"In this paper, we explore to conduct text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective -- ideal document representations should lead to very close results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations must be adaptive to the given class names. We propose a novel framework X-Class to realize it. Specifically, we first estimate comprehensive class representations by incrementally adding the most similar word to each class until inconsistency appears. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized token representations. We then cluster and align the documents to classes with the prior of each document assigned to its nearest class. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets.","tags":null,"title":"X-Class: Text Classification with Extremely Weak Supervision","type":"publication"},{"authors":["Sahil Chopra","Xinyang Zhang","Jingbo Shang"],"categories":null,"content":"","date":1600214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600214400,"objectID":"b2321277f07d924a90b2bdec0728ae1b","permalink":"https://schopra6.github.io/publication/meta/","publishdate":"2020-09-16T00:00:00Z","relpermalink":"/publication/meta/","section":"publication","summary":"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as “seed motifs”, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.","tags":null,"title":"META: Metadata-Empowered Weak Supervision for Text Classification","type":"publication"},{"authors":["Sahil Chopra","Jingbo Shang"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8d08b4d40973ce39b689e8b6008f4b35","permalink":"https://schopra6.github.io/publication/conwea/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/conwea/","section":"publication","summary":"Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.","tags":null,"title":"Contextualized Weak Supervision for Text Classification","type":"publication"},{"authors":["Rahul Wadbude","Vivek Gupta","Sahil Chopra","Harish Karnick"],"categories":null,"content":"","date":1515283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515283200,"objectID":"3106506114df20382093264727ae04e9","permalink":"https://schopra6.github.io/publication/ubr/","publishdate":"2018-01-07T00:00:00Z","relpermalink":"/publication/ubr/","section":"publication","summary":"Review score prediction of text reviews has recently gained a lot of attention in recommendation systems. A major problem in models for review score prediction is the presence of noise due to user-bias in review scores. We propose two simple statistical methods to remove such noise and improve review score prediction. Compared to other methods that use multiple classifiers, one for each user, our model uses a single global classifier to predict review scores. We empirically evaluate our methods on two major categories (Electronics and Movies and TV) of the SNAP published Amazon e-Commerce Reviews data-set and Amazon Fine Food reviews data-set. We obtain improved review score prediction for three commonly used text feature representations.","tags":null,"title":"User Bias Removal in Review Score Prediction","type":"publication"},{"authors":["Sahil Chopra","Vivek Gupta","Bhargavi Paranjape","Harish Karnick"],"categories":null,"content":"","date":1504742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504742400,"objectID":"f4fca25bad8544c7a9afffeabfb7cc69","permalink":"https://schopra6.github.io/publication/scdv/","publishdate":"2017-09-07T00:00:00Z","relpermalink":"/publication/scdv/","section":"publication","summary":"We present a feature vector formation technique for documents - Sparse Composite Document Vector (SCDV) which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In SCDV, word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex, multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks, we outperform the previous state-of-the-art method, NTSG (Liu et al., 2015a). We also show that SCDV embeddings perform well on heterogeneous tasks like Topic Coherence, context-sensitive Learning and Information Retrieval. Moreover, we achieve significant reduction in training and prediction times compared to other representation methods. SCDV achieves best of both worlds - better performance with lower time and space complexity","tags":null,"title":"SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations","type":"publication"},{"authors":["Sahil Chopra","Vivek Gupta","Purushottam Kar","Harish Karnick"],"categories":null,"content":"","date":1494115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494115200,"objectID":"1e6410e36bf161434bd2326d47453b6f","permalink":"https://schopra6.github.io/publication/bopt/","publishdate":"2017-05-07T00:00:00Z","relpermalink":"/publication/bopt/","section":"publication","summary":"Hierarchical classification is supervised multi-class classification problem over the set of class labels organized according to a hierarchy. In this project, we study the work by Ramaswamy et al. on hierarchical classification over symmetric tree distance loss. We extend the consistency of hierarchical classification algorithm over asymmetric tree distance loss. We design a O(nk log n) algorithm to find bayes optimal classification for a k-ary tree as hierarchy. We show that under reasonable assumptions over asymmetric loss function, the Bayes optimal classification over this asymmetric loss can be found in O(k log n). We exploit this insight and attempt to extend the Ova-Cascade algorithm Ramaswamy et al. for hierarchical classification over asymmetric loss","tags":null,"title":"Bayes-optimal Hierarchical Classification over Asymmetric Tree-Distance Loss","type":"publication"}]